'''
    HOW TO USE
    1. change dir on your comp
    2. define how many gpu that you will use at multi_gpu_model function
'''

import os
from os import listdir, makedirs
from os.path import join, exists, expanduser

import keras
from keras import optimizers
from keras import applications
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers
from keras.models import Sequential, Model
from keras.layers import Dense, GlobalAveragePooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras import backend as K
from keras.utils import multi_gpu_model
import tensorflow as tf
from tensorflow import keras
from tensorflow.python import distribute
from tensorflow.python.client import device_lib
from tensorflow.python.ops.control_flow_ops import with_dependencies

import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

# gpus = tf.config.experimental.list_physical_devices('GPU')

# if gpus:
#     try:
#         for gpu in gpus:
#             tf.config.experimental.set_memory_growth(gpu, True)

#         logical_gpus = tf.config.experimental.list_logical_devices('GPU')
#         print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")

#     except RuntimeError as e:
#         # Memory growth must be set before GPUs have been initialized
#         print(e)
#         print('shit!!')

physical_devices = tf.config.experimental.list_physical_devices('GPU')
if len(physical_devices) > 0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
    tf.config.experimental.set_memory_growth(physical_devices[1], True)
    # tf.config.experimental.set_memory_growth(physical_devices[2], True)




'''
    READING IMAGES
    - DIMENSIONS = (224,224)
'''

# dimensions of our images.
img_width, img_height = 224, 224 # we set the img_width and img_height according to the pretrained models we are
# going to use. The input size for ResNet-50 is 224 by 224 by 3.

train_data_dir = 'Training/'
validation_data_dir = 'Test/'
nb_train_samples = 7000
nb_validation_samples = 3000
batch_size = 30

'''
    DATA AUGMENTATION
    - all values are randomly generated by user
'''

train_datagen = ImageDataGenerator(
    rescale=1. / 255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical')

validation_generator = test_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='categorical')

'''
    ARCHITECTURES THAT WE USE :
    Both using ResNet50 but different network for the last part
'''
#import inception with pre-trained weights. do not include fully #connected layers
inception_base = applications.ResNet50(weights='imagenet', include_top=False,input_shape=(img_width, img_height, 3))



# add a global spatial average pooling layer
x = inception_base.output
x = GlobalAveragePooling2D()(x)
# add a fully-connected layer
x = Dense(512, activation='relu')(x)
# and a fully connected output/classification layer
predictions = Dense(30, activation='softmax')(x)
# create the full network so we can train on it
inception_transfer = Model(inputs=inception_base.input, outputs=predictions)

######################################################################################

#import inception with pre-trained weights. do not include fully #connected layers
inception_base_vanilla = applications.ResNet50(weights=None, include_top=False, input_shape=(img_width, img_height,3))



# add a global spatial average pooling layer
x = inception_base_vanilla.output
x = GlobalAveragePooling2D()(x)
# add a fully-connected layer
x = Dense(512, activation='relu')(x)
# and a fully connected output/classification layer
predictions = Dense(30, activation='softmax')(x)
# create the full network so we can train on it
inception_transfer_vanilla = Model(inputs=inception_base_vanilla.input, outputs=predictions)

inception_transfer_vanilla.summary()
inception_transfer.summary()


'''
    LIST ALL THE GPU DEVICES, ASSIGN TO GPUS AS WELL AND COMPILE
'''

# from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

strategy = tf.distribute.MirroredStrategy()
# model_inception_transfer = multi_gpu_model(inception_transfer, gpus=2)
# model_inception_transfer_vanilla = multi_gpu_model(inception_transfer_vanilla, gpus=2)

# from tensorflow.python.client import device_lib
# print(device_lib.list_local_devices())

with strategy.scope():

    opt = optimizers.SGD(lr=0.001, momentum=0.9)
    inception_transfer.compile(loss='categorical_crossentropy',
                # optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),
                optimizer=opt,
                metrics=['accuracy'])

    opt1 = optimizers.SGD(lr=1e-4, momentum=0.9)
    inception_transfer_vanilla.compile(loss='categorical_crossentropy',
                optimizer=opt1,
                metrics=['accuracy'])



'''
    TRAINING AND VALIDATION THE PRETRAINED MODEL
'''



history_inception_transfer = inception_transfer.fit_generator(train_generator,
                                steps_per_epoch= nb_train_samples / batch_size,
                                epochs=100,
                                validation_data=validation_generator,
                                validation_steps=nb_validation_samples /  batch_size)
inception_transfer.save('newanimal.h5')



history_inception_transfer_vanilla = inception_transfer_vanilla.fit_generator(train_generator,
                                steps_per_epoch= nb_train_samples / batch_size,
                                epochs=100,
                                validation_data=validation_generator,
                                validation_steps=nb_validation_samples /  batch_size)


inception_transfer_vanilla.save('animalsatu.h5')




'''
    PLOT THE GRAPH
'''

import matplotlib.pyplot as plt
# summarize history for accuracy
plt.plot(history_inception_transfer.history['val_acc'])
plt.plot(history_inception_transfer_vanilla.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['Pretrained', 'Vanilla'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history_inception_transfer.history['val_loss'])
plt.plot(history_inception_transfer_vanilla.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Pretrained', 'Vanilla'], loc='upper left')
plt.show()